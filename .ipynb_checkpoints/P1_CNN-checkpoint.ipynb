{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e307c075",
   "metadata": {},
   "source": [
    "# Práctica Classical Neuronal Networks\n",
    "\n",
    "* Alejandro Mayorga\n",
    "* Fernando Mondragón\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1521cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Primer paso: Cargar la database y distribuir las imágenes de entrenamiento y evaluación\n",
    "\n",
    "from tensorflow . keras . datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "( x_train , y_train ), ( x_test , y_test ) = fashion_mnist . load_data ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cf58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeramente, tenemos que preparar los datos de forma que sean fácilmente introducibles en nuestra red. Esto\n",
    "#implica reorganizar el array original en el que se nos es dada la información, así como para su tipo de datos a\n",
    "#float\n",
    "\n",
    "train_images = x_train.reshape((60000, 28 * 28))\n",
    "test_images = x_test.reshape((10000, 28 * 28))\n",
    "\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed81913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "#Creamos un tercer grupo de validación\n",
    "\n",
    "validation_images = train_images[:10000] # Copy of x_train from the begining (0) to 9999\n",
    "validation_labels = y_train[:10000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_images = train_images[10000:] # Copy of x_train from 10000 to the end\n",
    "train_labels = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5339621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos one-hot enconding para así disponer las etiquetas de forma que la red pueda entrenar\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "validation_labels = to_categorical(validation_labels)\n",
    "test_labels = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opciones a probar\n",
    "epoch_n = [10, 20, 30]\n",
    "batch_size_n = [32, 256, 512]\n",
    "\n",
    "layers_n = [1,2]\n",
    "neurons_n = [16,64,128]\n",
    "\n",
    "learning_rates = [0.1,0.01,0.001,0.0001]\n",
    "optimizers = [\"SGD\",\"Adam\"]\n",
    "\n",
    "\n",
    "\n",
    "#loss_function?\n",
    "#descenso de gradientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd274fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Ahora loopeamos por los hiperparametros\n",
    "#3*3*2*3*4*2 = 432 modelos.\n",
    "results = [0,\"\"]\n",
    "for epoch in epoch_n: #\n",
    "    for batch in batch_size_n:#\n",
    "        for layers in layers_n: # \n",
    "            for neurons in neurons_n: #\n",
    "                for learning_r in learning_rates:\n",
    "                    for optimizador in optimizers: #\n",
    "                        model = keras.Sequential()\n",
    "\n",
    "                        model.add(layers.Input(shape=(784, )))\n",
    "                        for i in range(layers):\n",
    "                            model.add(layers.Dense(neurons, activation=\"relu\"))\n",
    "\n",
    "                        model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "                        \n",
    "                        if optimizador ==\"SGD\":\n",
    "                            opt = keras.optimizers.SGD(learning_rate=learning_r)\n",
    "                        if optimizador ==\"Adam\":\n",
    "                            opt = keras.optimizers.Adam(learning_rate=learning_r)\n",
    "                        \n",
    "                        model.compile(optimizer=opt, \n",
    "                                  loss=\"categorical_crossentropy\",\n",
    "                                  metrics=[\"accuracy\"])\n",
    "\n",
    "                        print(len(train_images), len(train_labels))\n",
    "                        history =model.fit(train_images, train_labels, epochs=epoch, batch_size=batch, \n",
    "                            validation_data = (validation_images, validation_labels))\n",
    "                        \n",
    "                        history_dict = history.history\n",
    "                        results_val = history_dict['val_accuracy']\n",
    "                        \n",
    "                        if result[0]< result_val:\n",
    "                            result=[result_val,\"epoch: \"+ epoch+ \"\\n batch: \" +batch+ \"\\n layers: \" +layers+ \"\\n neurons: \" +neurons+ \"\\n learning rate: \" +learning rate+ \"\\n optimizador: \" +optimizador]                      \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af82387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407050 (1.55 MB)\n",
      "Trainable params: 407050 (1.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Input(shape=(784, )))\n",
    "\n",
    "model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d58624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n",
      "Epoch 1/5\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.5855 - accuracy: 0.7938 - val_loss: 0.4167 - val_accuracy: 0.8503\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.4018 - accuracy: 0.8540 - val_loss: 0.4148 - val_accuracy: 0.8432\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.8717 - val_loss: 0.3557 - val_accuracy: 0.8689\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3256 - accuracy: 0.8809 - val_loss: 0.3786 - val_accuracy: 0.8661\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.8865 - val_loss: 0.3517 - val_accuracy: 0.8704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa658709d50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "epoch_n = [10, 20, 30]\n",
    "batch_size_n = [32, 256, 512]\n",
    "#loss_function?\n",
    "#numero de neuronas\n",
    "#numero de capas?\n",
    "#descenso de gradientes\n",
    "#métricas? -> accuracy etc\n",
    "\n",
    "for x in range(1,10):\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", \n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    print(len(train_images), len(train_labels))\n",
    "    model.fit(train_images, train_labels, epochs=10, batch_size=128, \n",
    "              validation_data = (validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72828e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
